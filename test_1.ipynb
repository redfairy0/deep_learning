{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3\n",
    "epoch = 100\n",
    "\n",
    "imput_values = [[1,1,1],[1,0,1],[0,1,1],[0,0,1]]\n",
    "out_expected_values = [0, 1, 1, 0]\n",
    "\n",
    "\n",
    "neuron_hidden_layer_1 = [[0,0,0],[0,0,0],[0,0,0],[0,0,0]]\n",
    "neuron_hidden_layer_2 = [[0,0],[0,0],[0,0],[0,0]]\n",
    "out_actual_values = [0,0,0,0]\n",
    "\n",
    "neuron_hidden_layer_error_1 = [\n",
    "    [[0,0],[0,0],[0,0]],\n",
    "    [[0,0],[0,0],[0,0]],\n",
    "    [[0,0],[0,0],[0,0]],\n",
    "    [[0,0],[0,0],[0,0]]\n",
    "    ]\n",
    "neuron_hidden_layer_error_2 = [[0,0],[0,0],[0,0],[0,0]]\n",
    "\n",
    "weights_1 = [\n",
    "    [randint(0,1000)/1000,randint(0,1000)/1000,randint(0,1000)/1000],\n",
    "    [randint(0,1000)/1000,randint(0,1000)/1000,randint(0,1000)/1000],\n",
    "    [randint(0,1000)/1000,randint(0,1000)/1000,randint(0,1000)/1000]\n",
    "    ]\n",
    "\n",
    "weights_2 = [\n",
    "    [randint(0,1000)/1000,randint(0,1000)/1000],\n",
    "    [randint(0,1000)/1000,randint(0,1000)/1000]\n",
    "    ]\n",
    "\n",
    "weights_3 = [randint(0,1000)/1000,randint(0,1000)/1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculation_first_error(actual, expected):\n",
    "    return actual - expected\n",
    "\n",
    "def calculation_error(weight, delta):\n",
    "    return weight * delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculation_delta(value, error):\n",
    "    weights_delta = error * sigmoid(value) * (1-sigmoid(value))\n",
    "    return weights_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_weight(weight, value, delta, learning_rate):\n",
    "    return weight - value * delta * learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation_error(\n",
    "    neuron_hidden_layer_1,\n",
    "    neuron_hidden_layer_2,\n",
    "    out_actual_values,\n",
    "    out_expected_values,\n",
    "    weights_1,\n",
    "    weights_2,\n",
    "    weights_3,\n",
    "    learning_rate,\n",
    "    neuron_hidden_layer_error_1,\n",
    "    neuron_hidden_layer_error_2,\n",
    "    imput_values):\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(len(out_actual_values)):\n",
    "        first_error = calculation_first_error(out_actual_values[i], out_expected_values[i])\n",
    "        for j in range(len(neuron_hidden_layer_2[0])):\n",
    "            \n",
    "            delta = calculation_delta(neuron_hidden_layer_2[i][j], first_error)\n",
    "            weights_3[j] = correct_weight(weights_3[j], neuron_hidden_layer_2[i][j], delta, learning_rate)\n",
    "\n",
    "            neuron_hidden_layer_error_2[i][j] = calculation_error(weights_3[j], delta)\n",
    "    \n",
    "\n",
    "    for i in range(len(neuron_hidden_layer_2)):\n",
    "        for j in range(len(neuron_hidden_layer_1[0])):\n",
    "            for k in range(len(neuron_hidden_layer_error_2[0])):\n",
    "                for l in range(len(weights_2)):\n",
    "                    \n",
    "                    delta = calculation_delta(neuron_hidden_layer_1[i][j], neuron_hidden_layer_error_2[i][l])\n",
    "                    weights_2[k][l] = correct_weight(weights_2[k][l], neuron_hidden_layer_1[i][j], delta, learning_rate)\n",
    "\n",
    "                    neuron_hidden_layer_error_1[i][j][k] = calculation_error(weights_2[k][l], delta)\n",
    "\n",
    "\n",
    "    for i in range(len(neuron_hidden_layer_1)):\n",
    "        for j in range(len(imput_values[0])):\n",
    "            for k in range(len(neuron_hidden_layer_error_1[0][0])):\n",
    "\n",
    "                delta = calculation_delta(imput_values[i][j], neuron_hidden_layer_error_1[i][j][k])\n",
    "\n",
    "                for l in range(len(weights_1[0])):\n",
    "                \n",
    "                    weights_1[j][l] = correct_weight(weights_1[j][l], neuron_hidden_layer_1[i][j], delta, learning_rate)\n",
    "    \n",
    "    return weights_1, weights_2, weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running(imput_values, weights_1, weights_2, neuron_hidden_layer_1, neuron_hidden_layer_2,out_actual_values):\n",
    "    \n",
    "    for i in range(len(imput_values)):\n",
    "        for j in range(len(neuron_hidden_layer_1[0])):\n",
    "            summator_1 = 0\n",
    "            for l in range(len(weights_1)):\n",
    "                summator_1 += imput_values[i][l] * weights_1[l][j]\n",
    "            neuron_hidden_layer_1[i][j] = sigmoid(summator_1)\n",
    "\n",
    "    for i in range(len(neuron_hidden_layer_1)):\n",
    "        for j in range(len(neuron_hidden_layer_2[0])):\n",
    "            summator_2 = 0\n",
    "            for l in range(len(weights_2)):\n",
    "                summator_2 += neuron_hidden_layer_1[i][l] * weights_2[l][j]\n",
    "            neuron_hidden_layer_2[i][j] = sigmoid(summator_2)\n",
    "        \n",
    "    for i in range(len(neuron_hidden_layer_2)):\n",
    "        summator_3 = 0\n",
    "        for l in range(len(weights_3)):\n",
    "            summator_3 += neuron_hidden_layer_2[i][l] * weights_3[l]\n",
    "        out_actual_values[i] = sigmoid(summator_3)\n",
    "\n",
    "    return neuron_hidden_layer_1, neuron_hidden_layer_2, out_actual_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    epoch,\n",
    "    neuron_hidden_layer_1,\n",
    "    neuron_hidden_layer_2,\n",
    "    out_actual_values,\n",
    "    out_expected_values,\n",
    "    weights_1,\n",
    "    weights_2,\n",
    "    weights_3,\n",
    "    learning_rate,\n",
    "    neuron_hidden_layer_error_1,\n",
    "    neuron_hidden_layer_error_2,\n",
    "    imput_values):\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        neuron_hidden_layer_1, neuron_hidden_layer_2, out_actual_values = running(imput_values, weights_1, weights_2, neuron_hidden_layer_1, neuron_hidden_layer_2, out_actual_values)\n",
    "        print(out_actual_values)\n",
    "        weights_1, weights_2, weights_2 = backpropagation_error(\n",
    "            neuron_hidden_layer_1,\n",
    "            neuron_hidden_layer_2,\n",
    "            out_actual_values,\n",
    "            out_expected_values,\n",
    "            weights_1,\n",
    "            weights_2,\n",
    "            weights_3,\n",
    "            learning_rate,\n",
    "            neuron_hidden_layer_error_1,\n",
    "            neuron_hidden_layer_error_2,\n",
    "            imput_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.726352933485333, 0.7238137674247128, 0.7241495459289018, 0.7214114083756109]\n",
      "[0.5712254892636571, 0.5703338730670547, 0.5705986377090928, 0.5696643553933198]\n",
      "[0.5195930458005502, 0.5193023979280962, 0.519497766902772, 0.519205620382011]\n",
      "[0.5050343309299743, 0.504903260045776, 0.5050706996696429, 0.5049513521630747]\n",
      "[0.501480750809477, 0.5013746339104379, 0.5015262183845101, 0.5014353651262552]\n",
      "[0.5007597516501106, 0.5006435769211566, 0.5007816854466578, 0.5006807616693657]\n",
      "[0.5006572545377784, 0.5005241515535148, 0.5006490521862353, 0.5005295815797148]\n",
      "[0.5006644305646353, 0.5005145795926098, 0.5006263147538981, 0.5004873496658397]\n",
      "[0.5006843648282574, 0.500519744361174, 0.5006184955129661, 0.5004609511024833]\n",
      "[0.5007019487728385, 0.5005252195064045, 0.5006113557914524, 0.5004368380205722]\n",
      "[0.5007150225144176, 0.5005292627974021, 0.5006033336225979, 0.5004139201231015]\n",
      "[0.5007230203688576, 0.5005315796193344, 0.5005942945307285, 0.5003924576137011]\n",
      "[0.5007255818877221, 0.5005319321108443, 0.5005841342026961, 0.5003726638267368]\n",
      "[0.5007224630559826, 0.5005300432077262, 0.5005726780037117, 0.5003545895663647]\n",
      "[0.5007135598200062, 0.5005256239295209, 0.5005597055653978, 0.5003381360737167]\n",
      "[0.500698927493446, 0.500518408319757, 0.500544984964777, 0.5003230835827742]\n",
      "[0.5006787881452748, 0.5005081863275175, 0.5005283067466587, 0.5003091260870148]\n",
      "[0.5006535261078444, 0.5004948318692362, 0.5005095148456007, 0.5002959091762542]\n",
      "[0.5006236729852821, 0.5004783239817099, 0.5004885320130353, 0.5002830679190015]\n",
      "[0.5005898842063109, 0.50045875954265, 0.5004653778320642, 0.5002702618717687]\n",
      "[0.5005529094774052, 0.5004363566833452, 0.5004401780022446, 0.5002572046072971]\n",
      "[0.500513559539628, 0.5004114487449748, 0.5004131642970802, 0.5002436856864901]\n",
      "[0.5004726715149004, 0.5003844694074018, 0.5003846654074458, 0.5002295837399818]\n",
      "[0.5004310749237594, 0.5003559303852188, 0.5003550897172426, 0.5002148702159775]\n",
      "[0.500389560210238, 0.5003263937516375, 0.5003249018153423, 0.5001996042904644]\n",
      "[0.5003488513302011, 0.5002964414201678, 0.500294595122589, 0.5001839202993008]\n",
      "[0.5003095836374308, 0.5002666445083689, 0.5002646633135412, 0.5001680097174702]\n",
      "[0.5002722879254642, 0.5002375351893428, 0.5002355731902959, 0.5001521000885274]\n",
      "[0.5002373810542174, 0.5002095832202499, 0.500207741326171, 0.5001364333578133]\n",
      "[0.5002051631303431, 0.5001831786899181, 0.5001815162013666, 0.5001212458054212]\n",
      "[0.5001758207585242, 0.5001586217537995, 0.500157166803437, 0.5001067512789159]\n",
      "[0.5001494354857144, 0.5001361193428551, 0.5001348778831736, 0.5000931287934962]\n",
      "[0.5001259962664647, 0.5001157881521646, 0.500114751355025, 0.5000805149092306]\n",
      "[0.500105414615364, 0.5000976627143338, 0.5000968127962758, 0.500069000708223]\n",
      "[0.5000875410917687, 0.5000817070810022, 0.5000810216768645, 0.5000586327468332]\n",
      "[0.5000721818701899, 0.5000678285711709, 0.500067283846437, 0.5000494170813504]\n",
      "[0.5000591143573461, 0.5000558921630848, 0.5000554648871885, 0.5000413253581276]\n",
      "[0.500048101084498, 0.5000457343525946, 0.5000454031586713, 0.5000343019940215]\n",
      "[0.5000389013902455, 0.5000371756147383, 0.5000369216545111, 0.5000282716088161]\n",
      "[0.5000312806789179, 0.5000300309315757, 0.5000298381053855, 0.5000231460633642]\n",
      "[0.5000250172674274, 0.500024118145859, 0.5000239730540518, 0.5000188306656161]\n",
      "[0.5000199070048218, 0.5000192641404889, 0.5000191558685382, 0.5000152293013289]\n",
      "[0.5000157659603284, 0.500015309016733, 0.5000152288358206, 0.5000122484082015]\n",
      "[0.5000124315321809, 0.5000121085507662, 0.5000120495898004, 0.5000097998330443]\n",
      "[0.5000097623408776, 0.5000095352572432, 0.5000094921819707, 0.500007802691007]\n",
      "[0.5000076372489667, 0.5000074783934448, 0.5000074471131722, 0.5000061843889099]\n",
      "[0.5000059538070537, 0.5000058432120043, 0.5000058206237858, 0.5000048809890093]\n",
      "[0.5000046263729991, 0.5000045497270241, 0.500004533500129, 0.5000038370834962]\n",
      "[0.5000035840963557, 0.5000035312077485, 0.5000035196069139, 0.5000030053313697]\n",
      "[0.5000027689085456, 0.5000027325631927, 0.5000027243068261, 0.5000023457843167]\n",
      "[0.5000021336144445, 0.5000021087349754, 0.5000021028834505, 0.5000018251015276]\n",
      "[0.500001640144355, 0.5000016231764873, 0.500001619045496, 0.5000014157280491]\n",
      "[0.500001257997001, 0.5000012464653056, 0.5000012435595378, 0.5000010950890744]\n",
      "[0.5000009628834523, 0.5000009550721752, 0.500000953035144, 0.5000008448342824]\n",
      "[0.500000735567696, 0.5000007302930342, 0.5000007288695043, 0.5000006501520575]\n",
      "[0.5000005608905859, 0.5000005573392726, 0.500000556347383, 0.5000004991628044]\n",
      "[0.5000004269589254, 0.5000004245745183, 0.5000004238852671, 0.5000003823931221]\n",
      "[0.5000003244793104, 0.5000003228825457, 0.5000003224048037, 0.5000002923276772]\n",
      "[0.5000002462162131, 0.500000245149521, 0.5000002448191601, 0.5000002230326573]\n",
      "[0.5000001865548572, 0.5000001858439067, 0.5000001856159587, 0.5000001698431479]\n",
      "[0.5000001411512255, 0.5000001406783988, 0.5000001405214348, 0.5000001291062518]\n",
      "[0.5000001066536454, 0.5000001063398245, 0.5000001062319446, 0.5000000979718973]\n",
      "[0.5000000804825787, 0.500000080274691, 0.5000000802006777, 0.5000000742238148]\n",
      "[0.5000000606573201, 0.5000000605198569, 0.5000000604691632, 0.5000000561439079]\n",
      "[0.5000000456602266, 0.5000000455694878, 0.5000000455348217, 0.5000000424040904]\n",
      "[0.5000000343307667, 0.5000000342709698, 0.5000000342472997, 0.5000000319804931]\n",
      "[0.5000000257831378, 0.5000000257437948, 0.5000000257276564, 0.5000000240857516]\n",
      "[0.5000000193424112, 0.500000019316566, 0.5000000193055787, 0.5000000181157948]\n",
      "[0.5000000144951814, 0.5000000144782292, 0.5000000144707595, 0.5000000136082018]\n",
      "[0.5000000108515247, 0.5000000108404224, 0.5000000108353514, 0.5000000102097271]\n",
      "[0.5000000081157441, 0.5000000081084841, 0.5000000081050464, 0.5000000076510603]\n",
      "[0.5000000060639236, 0.5000000060591834, 0.5000000060568565, 0.5000000057272727]\n",
      "[0.5000000045267383, 0.5000000045236482, 0.5000000045220755, 0.500000004282706]\n",
      "[0.5000000033763193, 0.5000000033743082, 0.5000000033732469, 0.5000000031993344]\n",
      "[0.500000002516228, 0.5000000025149212, 0.5000000025142062, 0.5000000023878122]\n",
      "[0.5000000018738254, 0.5000000018729777, 0.5000000018724968, 0.5000000017806155]\n",
      "[0.500000001394465, 0.5000000013939161, 0.5000000013935934, 0.5000000013267897]\n",
      "[0.5000000010370875, 0.5000000010367328, 0.5000000010365164, 0.5000000009879417]\n",
      "[0.5000000007708776, 0.5000000007706488, 0.5000000007705041, 0.500000000735184]\n",
      "[0.5000000005727363, 0.500000000572589, 0.5000000005724924, 0.5000000005468123]\n",
      "[0.5000000004253673, 0.5000000004252726, 0.5000000004252083, 0.5000000004065404]\n",
      "[0.5000000003158342, 0.5000000003157735, 0.5000000003157308, 0.5000000003021637]\n",
      "[0.5000000002344714, 0.5000000002344326, 0.5000000002344043, 0.5000000002245478]\n",
      "[0.5000000001740655, 0.5000000001740408, 0.500000000174022, 0.5000000001668643]\n",
      "[0.5000000001292374, 0.5000000001292216, 0.5000000001292093, 0.500000000124014]\n",
      "[0.5000000000959806, 0.5000000000959706, 0.5000000000959625, 0.5000000000921938]\n",
      "[0.5000000000713133, 0.500000000071307, 0.5000000000713016, 0.5000000000685697]\n",
      "[0.5000000000530189, 0.5000000000530149, 0.5000000000530115, 0.5000000000510325]\n",
      "[0.50000000003945, 0.5000000000394474, 0.5000000000394451, 0.5000000000380128]\n",
      "[0.5000000000293839, 0.5000000000293824, 0.5000000000293808, 0.500000000028345]\n",
      "[0.5000000000219138, 0.5000000000219128, 0.5000000000219118, 0.5000000000211634]\n",
      "[0.500000000016367, 0.5000000000163664, 0.5000000000163657, 0.5000000000158256]\n",
      "[0.5000000000122452, 0.5000000000122449, 0.5000000000122444, 0.500000000011855]\n",
      "[0.5000000000091794, 0.5000000000091792, 0.500000000009179, 0.5000000000088984]\n",
      "[0.5000000000068965, 0.5000000000068964, 0.5000000000068962, 0.5000000000066944]\n",
      "[0.5000000000051941, 0.500000000005194, 0.5000000000051938, 0.5000000000050489]\n",
      "[0.5000000000039224, 0.5000000000039224, 0.5000000000039223, 0.5000000000038182]\n",
      "[0.5000000000029707, 0.5000000000029707, 0.5000000000029706, 0.500000000002896]\n",
      "[0.5000000000022571, 0.500000000002257, 0.500000000002257, 0.5000000000022036]\n",
      "[0.5000000000017204, 0.5000000000017204, 0.5000000000017204, 0.5000000000016822]\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    epoch,\n",
    "    neuron_hidden_layer_1,\n",
    "    neuron_hidden_layer_2,\n",
    "    out_actual_values,\n",
    "    out_expected_values,\n",
    "    weights_1,\n",
    "    weights_2,\n",
    "    weights_3,\n",
    "    learning_rate,\n",
    "    neuron_hidden_layer_error_1,\n",
    "    neuron_hidden_layer_error_2,\n",
    "    imput_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a67bc2351efb885807d5af97f0dafe594ff872815f10f8f5920ea914484c279b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
